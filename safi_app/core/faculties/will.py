"""
Defines the WillGate class.
"""
from __future__ import annotations
import json
import logging
from typing import List, Dict, Any, Tuple, Optional

# --- Import sibling (faculties) and parent (core) utilities ---
from ...utils import dict_sha256
from .utils import _norm_label as normalize_text

# --- Import services for type hinting and runtime ---
from typing import TYPE_CHECKING
from ..services.llm_provider import LLMProvider


class WillGate:
    """
    An ethical gatekeeper that evaluates a draft response against values.
    
    This class is responsible for *assembling the policy* based on the
    persona's profile and value set, and then *delegating* the LLM call
    to the LLMProvider to get an "approve" or "violation" decision.
    """

    def __init__(
        self,
        llm_provider: "LLMProvider",
        *,
        values: List[Dict[str, Any]],
        profile: Optional[Dict[str, Any]] = None,
        prompt_config: Optional[Dict[str, Any]] = None,
    ):
        """
        Initializes the WillGate.

        Args:
            llm_provider: An instance of the LLMProvider service.
            values: The list of value dictionaries for this persona.
            profile: The persona profile configuration.
            prompt_config: The configuration for system prompts and instructions.
        """
        self.llm_provider = llm_provider
        self.values = values
        self.profile = profile or {}
        self.prompt_config = prompt_config or {}
        self.cache: Dict[str, Tuple[str, str]] = {}
        self.log = logging.getLogger(self.__class__.__name__)

    def _key(self, x_t: str, a_t: str) -> str:
        """
        Creates a unique cache key for a given prompt and answer pair
        based on their normalized content and the active value set.
        """
        return dict_sha256(
            {"x": normalize_text(x_t), "a": normalize_text(a_t), "V": self.values}
        )

    async def evaluate(self, *, user_prompt: str, draft_answer: str) -> Tuple[str, str]:
        """
        Evaluates a draft answer for alignment with ethical rules and values.
        
        Args:
            user_prompt: The user's original prompt.
            draft_answer: The draft answer generated by the Intellect.

        Returns:
            A tuple of (decision, reason), e.g., ("approve", "Response is aligned.").
        """
        # Check cache first for this exact prompt/answer/value set
        key = self._key(user_prompt, draft_answer)
        if key in self.cache:
            decision, reason = self.cache[key]
            return decision, reason

        # --- 1. Policy Assembly ---
        # Get the specific "Will Rules" from the persona profile
        rules = self.profile.get("will_rules") or []
        name = self.profile.get("name", "")

        # If no specific rules are defined, create a default rule
        if not rules:
            joined = ", ".join(v["value"] for v in self.values)
            rules = [
                f"Do not approve drafts that reduce alignment with the declared values: {joined}."
            ]

        # Assemble the full system prompt for the Will model
        policy_parts = [
            self.prompt_config.get("header", "You are Will, the ethical gatekeeper."),
            f"Tradition: {name}" if name else "",
            "Rules:",
            *[f"- {r}" for r in rules],
            "Value Set:",
            json.dumps(self.values, indent=2),
            self.prompt_config.get(
                "footer",
                "Return a single JSON object with keys: decision, reason.",
            ),
        ]
        policy = "\n".join(filter(None, policy_parts))
        
        # Assemble the user prompt for the Will model
        prompt = f"Prompt:\n{user_prompt}\n\nDraft Answer:\n{draft_answer}"
        
        # --- 2. Delegate LLM Call ---
        try:
            # Delegate the LLM call and JSON parsing to the provider
            decision, reason = await self.llm_provider.get_will_decision(
                system_prompt=policy,
                user_prompt=prompt
            )

            # Cache the successful result
            self.cache[key] = (decision, reason)
            return decision, reason
            
        except Exception as e:
            self.log.exception(f"WillGate delegation failed: {e}") 
            return ("violation", "Internal evaluation error")