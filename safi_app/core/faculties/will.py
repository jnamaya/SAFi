"""
Defines the WillGate class.

This is the "security guard" faculty, a hardened ethical gatekeeper
that evaluates a draft response against a set of non-negotiable rules.
"""
from __future__ import annotations
import json
import logging
from typing import List, Dict, Any, Tuple, Optional

# --- Import sibling (faculties) and parent (core) utilities ---
# `dict_sha256` is imported from the app-level `utils.py`
from ...utils import dict_sha256
# `normalize_text` is imported from the local `faculties/utils.py`
from .utils import _norm_label as normalize_text

# --- Import services for type hinting and runtime ---
from typing import TYPE_CHECKING
from ..services.llm_provider import LLMProvider


class WillGate:
    """
    An ethical gatekeeper that evaluates a draft response against values.
    
    This class is responsible for *assembling the policy* based on the
    persona's profile and value set, and then *delegating* the LLM call
    to the LLMProvider to get an "approve" or "violation" decision.
    
    """

    def __init__(
        self,
        llm_provider: "LLMProvider",
        *,
        values: List[Dict[str, Any]],
        profile: Optional[Dict[str, Any]] = None,
        prompt_config: Optional[Dict[str, Any]] = None,
    ):
        """
        Initializes the WillGate.

        Args:
            llm_provider: An instance of the LLMProvider service.
            values: The list of value dictionaries for this persona.
            profile: The persona profile configuration.
            prompt_config: The configuration for system prompts and instructions.
        """
        self.llm_provider = llm_provider
        self.values = values
        self.profile = profile or {}
        self.prompt_config = prompt_config or {}
        self.cache: Dict[str, Tuple[str, str]] = {}
        self.log = logging.getLogger(self.__class__.__name__)

    def _key(self, x_t: str, a_t: str) -> str:
        """
        Creates a unique cache key for a given prompt and answer pair
        based on their normalized content and the active value set.
        
        Args:
            x_t: The user's original prompt.
            a_t: The AI's draft answer.
            
        Returns:
            A SHA256 hash representing this unique request.
        """
        return dict_sha256(
            {"x": normalize_text(x_t), "a": normalize_text(a_t), "V": self.values}
        )

    async def evaluate(self, *, user_prompt: str, draft_answer: str) -> Tuple[str, str]:
        """
        Evaluates a draft answer for alignment with ethical rules and values.
        
        Args:
            user_prompt: The user's original prompt. (Used for caching key).
            draft_answer: The draft answer generated by the Intellect.

        Returns:
            A tuple of (decision, reason), e.g., ("approve", "Response is aligned.").
        """
        # The cache key *does* depend on the user_prompt, to ensure that
        # a draft is evaluated within the context of the prompt that created it.
        key = self._key(user_prompt, draft_answer)
        if key in self.cache:
            decision, reason = self.cache[key]
            self.log.info(f"WillGate cache hit for key: {key[:10]}...")
            return decision, reason
        
        self.log.info(f"WillGate cache miss. Evaluating draft...")

        # --- 1. Policy Assembly ---
        # Get the specific "Will Rules" from the persona profile
        rules = self.profile.get("will_rules") or []
        name = self.profile.get("name", "")

        # If no specific rules are defined, create a default rule
        if not rules:
            joined = ", ".join(v["value"] for v in self.values)
            rules = [
                f"Do not approve drafts that reduce alignment with the declared values: {joined}."
            ]

        # Assemble the full system prompt for the Will model
        policy_parts = [
            self.prompt_config.get("header", "You are Will, the ethical gatekeeper."),
            f"Tradition: {name}" if name else "",
            "Rules:",
            *[f"- {r}" for r in rules],
            "Value Set (for context, not rules):",
            json.dumps(self.values, indent=2),
            self.prompt_config.get(
                "footer",
                "Your task is to analyze the 'Draft Answer' below. "
                "Based *only* on the rules provided, return a single JSON object "
                "with keys: 'decision' ('approve' or 'violation') and 'reason'.",
            ),
        ]
        policy_system_prompt = "\n".join(filter(None, policy_parts))
        
        # --- 2. Create the "Blinded" Prompt ---
        # The Will model is *only* shown the draft answer.
        # It is *never* shown the user_prompt, closing the jailbreak vector.
        prompt_to_llm = f"Draft Answer:\n{draft_answer}"
        
        # --- 3. Delegate LLM Call ---
        try:
            # Delegate the LLM call and JSON parsing to the provider
            decision, reason = await self.llm_provider.get_will_decision(
                system_prompt=policy_system_prompt,
                user_prompt=prompt_to_llm  # <-- This 'prompt_to_llm' variable *only* contains the draft
            )

            # Cache the successful result
            self.cache[key] = (decision, reason)
            return decision, reason
            
        except Exception as e:
            self.log.exception(f"WillGate delegation failed: {e}") 
            return ("violation", "Internal evaluation error")